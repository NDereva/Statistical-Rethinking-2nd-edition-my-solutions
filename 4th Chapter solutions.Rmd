---
title: "4th chapter Solutions"
author: "Mykola Dereva"
date: "1/20/2021"
output: html_document
---
```{r}
rm(list = ls())

library(rethinking)
library(ggplot2)
library(tidyverse)
```

4E1. In the model definition below, which line is the likelihood?
1)  yi ∼ Normal(µ, σ)
2)  µ ∼ Normal(0, 10)
3)  σ ∼ Exponential(1)

Answer: First option is likelihood, second and third - prior probability 
distributions.

4E2. In the model definition just above, how many parameters are in the posterior distribution?

Answer: Two - µ and σ


4E4. In the model definition below, which line is the linear model?
      yi ∼ Normal(µ, σ)
      µi = α + βxi
      α ∼ Normal(0, 10)
      β ∼ Normal(0, 1)
      σ ∼ Exponential(2)
      
Answer: second line specifies linear model (µi = α + βxi)

4E5. In the model definition just above, how many parameters are in the posterior distribution?

Answer: There are three parameters in the posterior distribution: α, β, σ.
The mean µ is no longer a parameter, since it is described with deterministic
relationship.

4M1. For the model definition below, simulate observed y values from the prior (not the posterior).
          yi ∼ Normal(µ, σ)
          µ ∼ Normal(0, 10)
          σ ∼ Exponential(1)
          
```{r}
mean <- rnorm(1e4, 0, 10)
sd <- rexp(1e4, 1)

y <- rnorm(1e4, mean = mean, sd = sd)

dens(y)
```

4M2. Translate the model just above into a quap formula.

```{r}
model_spec_1 <- alist(
  y ~ dnorm(mean, sigma),
  mean ~ dnorm(0, 10),
  sigma ~ dexp(1)
)
```


4M3. Translate the quap model formula below into a mathematical model definition.
      y ~ dnorm( mu , sigma ),
      mu <- a + b*x,
      a ~ dnorm( 0 , 10 ),
      b ~ dunif( 0 , 1 ),
      sigma ~ dexp( 1 )
      
Answer:

$$
y_{i} \sim ~ Normal(mu_{i}, sigma) \\
mu_{i} = a + bx_{i} \\
a \sim ~ Normal(1, 10) \\
b \sim ~ Uniform(0, 1) \\
sigma \sim ~ Exponential(1)
$$

4M4. A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.

$$
height_{i} \sim ~ Normal(mean_{i}, sdev) \\
mean_{i} = a + b (x_{i} - \bar{x}) \\
a \sim ~ Normal(160, 25) \\
b \sim ~ Exponential(1) \\
sdev \sim ~ Uniform(0, 20)
$$

Since we try to predict height of a student based on the year of study I have defined
the linear relationship between those variables. 

I assume that the height of a students is normally distributed. I assume that 
the average height of a teenage student is 160cm and the 95% of a students have
height between 110 and 210cm. I just come up with this numbers but they seems 
to be quite possible to me. 
Also I assume that students still grow and I expect slope coef (b) to be positive



Prior predictive check

```{r}
set.seed(42)

year <- c(1, 2, 3)
a <- rnorm(5e2, 160, 25)
b <- rexp(5e2, 0.5)

prior <- expand.grid(a = a, b = b, year = year)
prior$height <- prior$a + prior$b * prior$year
prior <- tibble(prior)

prior %>% 
  ggplot(aes(x = height, col = factor(year))) +
  geom_density(alpha = 0.5) +
  theme_minimal()
```

4M5. Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?

I already assumed they they grow each year. 


4M6. Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?

the standard deviation is the sqrt of a variance. 
so I would redefine the parameter of standard deviation to lognorm(0, 1)
in this case there is very little prob to have stdev higher than 8

```{r}
dens(rlnorm(1e4, 0, 1), show.HPDI = 0.95, xlim = c(0, 10))
```


4M7. Refit model m4.3 from the chapter, but omit the mean weight xbar this time.
Compare the new model’s posterior to that of the original model. In particular,
look at the covariance among the parameters. What is different?
Then compare the posterior predictions of both models


Original m4.3 model
```{r}
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18,]

# define the average weight, x-bar
xbar <- mean(d2$weight)


m4.3 <- quap(
    alist(
    height ~ dnorm(mu , sigma) ,
    mu <- a + b*(weight - xbar) ,
    a ~ dnorm(178, 20) ,
    b ~ dlnorm(0 , 1) ,
    sigma ~ dunif(0, 50)
  ) , data = d2)

```

```{r}
precis(m4.3)
```

```{r}
vcov(m4.3) %>% round(3)
```

Model without xbar
```{r}
m4.3a <- quap(
    alist(
    height ~ dnorm(mu , sigma) ,
    mu <- a + b*weight,
    a ~ dnorm(178, 20) ,
    b ~ dlnorm(0 , 1) ,
    sigma ~ dunif(0, 50)
  ) , data = d2)
```

```{r}
precis(m4.3a)
```


```{r}
vcov(m4.3a) %>% round(3)
```


Regarding the parameters, the estimate of b and sigma virtually the same. 
"a" parameter is differ between the models. 
In the model with centring (m4.3) we can interpret it as the mean height in a sample
In the second model (m4.3a) we can interpret it as a mean height when the weight is 0.
the fitted lines should be the same. 

Lets verify that the lines are the same 


```{r}
weight.seq <- seq(from = min(d2$weight) , to = max(d2$weight) , by = 1)

mu <- link(m4.3 , data = data.frame(weight = weight.seq))

# summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)

# two plots side by side
par(mfrow = c(1,2))
    
# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data = d2 , col = col.alpha(rangi2,0.5),
     main = "With Centring")

# plot the MAP line, aka the mean mu for each weight
lines(weight.seq, mu.mean)

# plot a shaded region for 89% PI
shade(mu.PI, weight.seq)

# Second Plot 

mu <- link(m4.3a, data = data.frame(weight = weight.seq))

# summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)

# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data = d2 , col = col.alpha(rangi2,0.5),
     main = "Without Centring")

# plot the MAP line, aka the mean mu for each weight
lines(weight.seq, mu.mean)

# plot a shaded region for 89% PI
shade(mu.PI, weight.seq)

```


4M8. In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots and observe what happens to the resulting spline. Then adjust also the width of the prior on the weights—change the standard deviation of the prior and watch what happens. What do you think the combination of knot number and the prior on the weights controls?

```{r}
library(splines)
```


```{r}
data(cherry_blossoms)
d <- cherry_blossoms
d2 <- d[ complete.cases(d$doy) , ] # complete cases on doy

```

```{r}
num_knots <- 15
knot_list <- quantile( d2$year , probs = seq(0, 1, length.out = num_knots) )

B <- bs(d2$year,
  knots = knot_list[-c(1,num_knots)],
  degree = 3, intercept = TRUE)
```


```{r}
splines_15 <- quap(
  alist(
    D ~ dnorm(mu, sigma) ,
    mu <- a + B %*% w ,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ), data = list(D = d2$doy, B = B) ,
  start = list(w = rep(0, ncol(B) ))
)
```


```{r}
num_knots <- 30
knot_list <- quantile( d2$year , probs = seq(0, 1, length.out = num_knots) )

B <- bs(d2$year,
  knots = knot_list[-c(1,num_knots)],
  degree = 3, intercept = TRUE)

splines_30 <- quap(
  alist(
    D ~ dnorm(mu, sigma) ,
    mu <- a + B %*% w ,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ), data = list(D = d2$doy, B = B) ,
  start = list(w = rep(0, ncol(B) ))
)
```


```{r}
num_knots <- 30
knot_list <- quantile( d2$year , probs = seq(0, 1, length.out = num_knots) )

B <- bs(d2$year,
  knots = knot_list[-c(1,num_knots)],
  degree = 3, intercept = TRUE)

splines_30_narrow_w <- quap(
  alist(
    D ~ dnorm(mu, sigma) ,
    mu <- a + B %*% w ,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data = list(D = d2$doy, B = B) ,
  start = list(w = rep(0, ncol(B) ))
)
```


plot model with 15 and with 30 splines
```{r}
mu <- link(splines_15)
mu_PI <- apply(mu, 2, PI, 0.89)
plot(d2$year, d2$doy, col = col.alpha(rangi2,0.3) , pch = 16, 
     main = "15 Splines Model")
shade(mu_PI, d2$year, col = col.alpha("black", 0.3))
```

```{r}
mu <- link(splines_30)
mu_PI <- apply(mu, 2, PI, 0.89)
plot(d2$year, d2$doy, col = col.alpha(rangi2,0.3) , pch = 16,
     main = "30 Splines Model")
shade(mu_PI, d2$year, col = col.alpha("black", 0.3))
```

```{r}
mu <- link(splines_30_narrow_w)
mu_PI <- apply(mu, 2, PI, 0.89)
plot(d2$year, d2$doy, col = col.alpha(rangi2,0.3) , pch = 16,
     main = "30 Splines Model with a narrow priors for w")
shade(mu_PI, d2$year, col = col.alpha("black", 0.3))
```


With the increase of number of splines the model became a bit wigglier
But small priors for 'w' restrict the wigglinness magnitude
so the model appear to be more straight



4H1. The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals for each of these individuals. That is, fill in the table below, using model-based predictions.
Individual weight expected height 89% interval

| Individual  | weight  | Expected height  | 89% interval |
|-------------|---------|------------------|--------------|
| 1           | 46.95   |                  |              |
| 2           | 43.72   |                  |              |
| 3           | 64.78   |                  |              |
| 4           | 32.59   |                  |              |
| 5           | 54.63   |                  |              |

We already estimated a model for the height so lets use it


```{r}
weights <- c(46.95, 43.72, 64.78, 32.59, 54.63)

sim.height <- sim(m4.3, data = list(weight = weights, n = 1e4))
sim.height <- data.frame(sim.height)
```

```{r}
sim.height %>% 
  pivot_longer(cols = everything()) %>% 
  group_by(name) %>% 
  summarise("Expected height" = median(value),
            "Lower 89% bound" = PI(value)[1],
            "Higher 89% bound" = PI(value)[2])


  
```



4H2. Select out all the rows in the Howell1 data with ages below 18 years of age.
If you do it right, you should end up with a new data frame with 192 rows in it.
  (a) Fit a linear regression to these data, using quap. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?
  (b) Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% interval for the mean. Also superimpose the 89% interval for predicted heights.
  (c) What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model
  
```{r}
data("Howell1")
d <- Howell1

d2 <- d %>% 
  filter(age < 18)

xbar <- mean(d2$weight)
```

Fit the model with quap
```{r}
specs <- alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b * (weight - xbar),
  a ~ dnorm(120, 30),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)
)

m_child <- quap(specs, data = d2)

precis(m_child)
```
According to the model the mean child weight given the data is 108cm
Increase in weight for 10kg is assosiated with the increase in height for 27cm
on average

 (b) plot the model with the prediction intervals
 
```{r}
# extract samples from the posterior
post <- extract.samples(m_child)

weight.seq <- seq(from = min(d2$weight) , to = max(d2$weight) , by = 1)

mu <- link(m_child , data = data.frame(weight = weight.seq))
# summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)


#prediction interval 
sim.height <- sim(m_child, data = list(weight = weight.seq), n = 1e4)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)


# plot raw data
plot(height ~ weight, d2, col = col.alpha(rangi2,0.5))

# draw MAP line
lines(weight.seq, mu.mean)

# draw HPDI region for line
shade(mu.PI, weight.seq)

# draw PI region for simulated heights
shade(height.PI, weight.seq)

```

(C)

The data cannot be desctibed strictly by a linear relationship. 
Thus, the model overestimate the height of an children
with the small and high weight and underestimate the height in the middle
of a sample. 

We should try to use some more flexible functional form. 
Or log transform the data. 



4H3. Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the logarithm of body weight that scales with height!” Let’s take your colleague’s advice and see what happens.
  (a) Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Can you interpret the resulting estimates?
  (b) Begin with this plot: plot( height ~ weight , data=Howell1 ). Then use samples
from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, 
(2) the 97% interval for the mean, and 
(3) the 97% interval for predicted heights.

```{r}
d <- d %>% 
  mutate(log_weight = log(weight))

lxbar <- mean(d$log_weight)
```

Quap Model

```{r}
specs <- alist(
  height ~ dnorm(mu, sigma),
  mu <- a + b * (log_weight - lxbar),
  a ~ dnorm(120, 30),
  b ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)
)

m_height <- quap(specs, data = d)

precis(m_height)
```

```{r}
weight.seq <- log(seq(from = 1 , to = 70 , by = 1))


mu <- link(m_height , data = data.frame(log_weight = weight.seq))
# summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.97)


#prediction interval 
sim.height <- sim(m_height, data = list(log_weight = weight.seq), n = 1e4)
height.PI <- apply(sim.height, 2, PI, prob = 0.97)


# plot raw data
plot(height ~ weight, d, col = col.alpha(rangi2,0.5))

weight.seq <- exp(weight.seq)

# draw MAP line
lines(weight.seq, mu.mean)

# draw HPDI region for line
shade(mu.PI, weight.seq)

# draw PI region for simulated heights
shade(height.PI, weight.seq)
```



# 4H4. 
Plot the prior predictive distribution for the parabolic polynomial regression model in the chapter. You can modify the code that plots the linear regression prior predictive distribution. Can you modify the prior distributions of α, β1, and β2 so that the prior predictions stay within the biologically reasonable outcome space? That is to say: Do not try to fit the data by hand. But do try to keep the curves consistent with what you know about height and weight, before seeing these exact data


Prior predictive distribution for polynomial model specified as in the chapter:

$$
height_{i} \sim ~ Normal(mean_{i}, sdev) \\
mean_{i} = a + b_{1}x_{i} + b_{2}x^2_{i} \\
a \sim ~ Normal(178, 20) \\
b_{1} \sim ~ Log-Normal(0, 1) \\
b_{2} \sim ~ Normal(0, 1) \\
sdev \sim ~ Uniform(0, 50)
$$


```{r}
n <- 1e3

a <- rnorm(n, 178, 20)
b1 <- rlnorm(n, 0, 1)
b2 <- rnorm(n, 0, 1)

weight.seq <- seq(from = 25, to = 70, length.out = 100)
```

```{r}
data.frame(a, b1, b2) %>% 
  mutate(group = seq(n)) %>% 
  expand(nesting(group, a, b1, b2),
         weight = weight.seq) %>% 
  mutate(height = a + b1*weight + b2*weight^2) %>% 
  ggplot(aes(x = weight, y = height, group = group)) +
  geom_line(alpha = 0.1) +
  ylim(c(0, 250))

```
As we can see the priors do not look great lets try to improve the results 



```{r}
n <- 1e3

a <- rnorm(n, -180, 10)
b1 <- rnorm(n, 10, 5)
b2 <- runif(n, -0.15, 0)
weight.seq <- seq(from = 25, to = 70, length.out = 100)


data.frame(a, b1, b2) %>% 
  mutate(group = seq(n)) %>% 
  expand(nesting(group, a, b1, b2),
         weight = weight.seq) %>% 
  mutate(height = a + b1*weight + b2*weight^2) %>% 
  ggplot(aes(x = weight, y = height, group = group)) +
  geom_line(alpha = 0.1) +
  ylim(c(0, 300))
```


It turned out that it is very complicated to guess normal priors in this case
The priors I set is not ideal but better then original one. 


# 4H5. 
Return to data(cherry_blossoms) and model the association between blossom date (doy)
and March temperature (temp). Note that there are many missing values in both variables. You may consider a linear model, a polynomial, or a spline on temperature. 
How well does temperature trend predict the blossom trend?


first lets look at the relationship between doy and temp

```{r}
data(cherry_blossoms)
cherry <- cherry_blossoms
```

```{r}
ggplot(cherry, aes(x = temp, y = doy)) +
  geom_point(alpha = 0.3)
```

the relationship appears to be linear but with the high variation

lets try to fit linear model

First, drop the missing values
```{r}
cherry <- cherry %>% 
  filter(!is.na(doy), !is.na(temp))
```




```{r}
cherry_lin_spec <- alist(
  doy ~ dnorm(mu, sigma),
  mu <- a + b * temp,
  # a is the average day of the year when cherry blossom
  a ~ dnorm(100, 25),
  # b is the change in doy with 1 degree change in temp
  # I expect negative relationship
  b ~ dnorm(-2, 5),
  sigma ~ dunif(0, 20)
)

cherry_lin_model <- quap(
  cherry_lin_spec, data = cherry
)
```

```{r}
precis(cherry_lin_model)
```

```{r}
# extract samples from the posterior
post <- extract.samples(cherry_lin_model)

temp.seq <- seq(from = min(cherry$temp) , to = max(cherry$temp) , length.out = 100)
mu <- link(cherry_lin_model , data = data.frame(temp = temp.seq))

# summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)


#prediction interval 
sim.temp <- sim(cherry_lin_model, data = list(temp = temp.seq), n = 1e4)
height.PI <- apply(sim.temp, 2, PI, prob = 0.89)

# plot raw data
plot(doy ~ temp, cherry, col = col.alpha(rangi2,0.5))

# draw MAP line
lines(temp.seq, mu.mean)

# draw HPDI region for line
shade(mu.PI, temp.seq)

# draw PI region for simulated heights
shade(height.PI, temp.seq)
```



# 4H8.
The cherry blossom spline in the chapter used an intercept α, but technically it doesn’t require one. The first basis functions could substitute for the intercept. Try refitting the cherry blossom spline without the intercept. 
What else about the model do you need to change to make this work?

Lets start from the original spline model from the book
```{r}
library(splines)
```


```{r}
data(cherry_blossoms)
d <- cherry_blossoms


d2 <- d[ complete.cases(d$doy) , ] # complete cases on doy
num_knots <- 15
knot_list <- quantile( d2$year , probs = seq(0, 1, length.out = num_knots) )


B <- bs(d2$year,
  knots = knot_list[-c(1,num_knots)],
  degree = 3, intercept = TRUE)
```

```{r}
cherry_splines_orig <- quap(
  alist(
    D ~ dnorm(mu, sigma) ,
    mu <- a + B %*% w ,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ), data = list(D = d2$doy, B = B) ,
  start = list(w = rep(0, ncol(B) ))
)
```

```{r}
mu <- link(cherry_splines_orig)
mu_PI <- apply(mu, 2, PI, 0.97)
plot(d2$year, d2$doy, col = col.alpha(rangi2,0.3) , pch = 16)
shade(mu_PI, d2$year, col = col.alpha("black", 0.3))
```

Lets try to make a model without an intercept 

```{r}
cherry_splines_no_intercept <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- B %*% w,
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ), data = list(D = d2$doy, B = B) ,
  start = list(w = rep(0, ncol(B) ))
)
```

```{r}
mu <- link(cherry_splines_no_intercept)
mu_PI <- apply(mu, 2, PI, 0.97)
plot(d2$year, d2$doy, col = col.alpha(rangi2,0.3) , pch = 16)
shade(mu_PI, d2$year, col = col.alpha("black", 0.3))
```

The models appear to be quite simmilar but the one without intercept 
seams to underestimate the datat in hte beggingin of the sample. 
This might happen because the intersept in the first model is around 100
but the priors for w are close to 0 
If we make the wider priors for w the model should be the same as the original one 
lets try this 



```{r}
cherry_splines_no_intercept_wide_w <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- B %*% w,
    w ~ dnorm(0, 100),
    sigma ~ dexp(1)
  ), data = list(D = d2$doy, B = B) ,
  start = list(w = rep(0, ncol(B) )),
  control = list(maxit = 5000)
)
```

```{r}
mu <- link(cherry_splines_no_intercept_wide_w)
mu_PI <- apply(mu, 2, PI, 0.97)
plot(d2$year, d2$doy, col = col.alpha(rangi2,0.3) , pch = 16)
shade(mu_PI, d2$year, col = col.alpha("black", 0.3))
```


With the very wide priors for w, model without intercept appears to be quite the
same as the one with the intercept